# DataEngineering Workspace Documentation

**Last Updated:** December 2, 2025  
**Repository:** DataEnginering  
**Owner:** Narayan-git  
**Branch:** main

---

## ğŸ“‹ Table of Contents

1. [Project Overview](#project-overview)
2. [Workspace Structure](#workspace-structure)
3. [Architecture & Technology Stack](#architecture--technology-stack)
4. [Module Breakdown](#module-breakdown)
5. [Data Pipeline Flow](#data-pipeline-flow)
6. [Key Concepts](#key-concepts)
7. [Implementation Details](#implementation-details)

---

## ğŸ¯ Project Overview

### Purpose
This is a **Databricks Lakehouse** project implementing a unified, scalable data pipeline for SportsBar order details. The project aims to integrate SportsBar's order data from diverse, unstructured sources (spreadsheets, cloud drives, APIs) into a structured, reliable data warehouse for the FMCG (Fast-Moving Consumer Goods) data analytics team.

### Primary Objective
Create a **single, reliable source** for product sales tracking and cross-company planning using the Medallion Architecture (Bronze â†’ Silver â†’ Gold layers).

### Target Use Case
- SportsBar order details tracking
- FMCG data analytics and reporting
- Product performance analysis
- Sales forecasting

---

## ğŸ“ Workspace Structure

```
DataEnginering/
â”œâ”€â”€ README.md                           # Root documentation
â”œâ”€â”€ WORKSPACE_DOCUMENTATION.md          # This file
â”œâ”€â”€ Databricks_project_1/              # Main project folder
â”‚   â”œâ”€â”€ README.MD                      # Project planning & architecture
â”‚   â”œâ”€â”€ 1_setup_catalog/               # Catalog initialization & utilities
â”‚   â”‚   â”œâ”€â”€ .setup.py                  # Setup script
â”‚   â”‚   â”œâ”€â”€ utilities.py               # Shared schema definitions
â”‚   â”‚   â””â”€â”€ dim_date_table_creation.py # Date dimension creation
â”‚   â”œâ”€â”€ 2_dimension_data_processing/   # Dimension table ETL
â”‚   â”‚   â”œâ”€â”€ 1_customer_data_processing.py      # Customer dimension
â”‚   â”‚   â”œâ”€â”€ 2_products_data_processing.py      # Product dimension
â”‚   â”‚   â””â”€â”€ 3_pricing_data_processing.py       # Pricing dimension
â”‚   â””â”€â”€ 3_fact_data_processing/        # Fact table ETL
â”‚       â”œâ”€â”€ 1_full_load_fact.py        # Full load of orders
â”‚       â””â”€â”€ 2_incremental_load_fact.py # Incremental load of orders
â”œâ”€â”€ factory/                           # Azure Data Factory configs
â”‚   â””â”€â”€ nsahu-de-adf.json
â”œâ”€â”€ integrationRuntime/                # Integration Runtime config
â”‚   â””â”€â”€ AutoResolveIntegrationRuntime.json
â”œâ”€â”€ managedVirtualNetwork/             # Virtual Network config
â”‚   â””â”€â”€ default.json
â””â”€â”€ Spark/                             # Spark setup documentation
    â””â”€â”€ PreReqInstallation/
        â”œâ”€â”€ InstallationSteps.txt
        â””â”€â”€ SparkOperation.txt
```

---

## ğŸ—ï¸ Architecture & Technology Stack

### Platform & Storage
| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Compute Platform** | Databricks (Free Edition) | Serverless compute, unified storage & processing |
| **Storage Layer** | Azure Data Lake Storage Gen2 (ADLS Gen2) | Cloud-native data storage |
| **Data Format** | Delta Lake | ACID compliance, schema enforcement, efficient incremental updates |
| **Processing Engine** | Apache Spark (Python/SQL) | Distributed data processing |
| **Orchestration** | Databricks Workflows | Scheduling & daily load management |

### Data Architecture Pattern
**Medallion Architecture** (3-layer model):
- **Bronze Layer:** Raw ingestion with minimal transformation
- **Silver Layer:** Cleaned, conformed, and deduplicated data
- **Gold Layer:** Business-ready, aggregated analytics data

---

## ğŸ“Š Module Breakdown

### 1ï¸âƒ£ Setup & Catalog (1_setup_catalog/)

#### `utilities.py`
**Purpose:** Centralized schema definitions and shared utilities  
**Key Definitions:**
```python
bronze_schema = "bronze"
silver_schema = "silver"
gold_schema = "gold"
```
- Provides schema naming conventions for all ETL jobs
- Used across all dimension and fact processing modules

#### `dim_date_table_creation.py`
**Purpose:** Create a dimensional date table for time-based analytics  
**Key Functionality:**
- Generates monthly grain dates from 2024-01-01 to 2025-12-01
- Creates derived columns:
  - `date_key` (YYYYMM format, e.g., 202401)
  - `year`, `month_name`, `month_short_name`
  - `quarter`, `year_quarter`
- Saves to `fmcg.gold.dim_date` Delta table
- Enables efficient joins for time-based reporting

**Output Schema:**
```
month_start_date: date
date_key: integer (yyyyMM)
year: integer
month_name: string
month_short_name: string
quarter: string (e.g., Q1)
year_quarter: string (e.g., 2024-Q1)
```

---

### 2ï¸âƒ£ Dimension Data Processing (2_dimension_data_processing/)

#### `1_customer_data_processing.py`
**Purpose:** Process and standardize customer dimension data  
**Data Source:** Azure ADLS Gen2 - `customers/*.csv`

**Processing Steps:**

| Layer | Transformations |
|-------|-----------------|
| **Bronze** | Load CSV with metadata (file_name, file_size, read_timestamp) |
| **Silver** | â€¢ Remove duplicates (by customer_id)<br>â€¢ Trim whitespace from names<br>â€¢ Standardize city names (fix misspellings)<br>â€¢ Handle null cities with business-confirmed mappings<br>â€¢ Apply proper casing to customer names<br>â€¢ Create composite "customer" field |
| **Gold** | Select essential columns, stage for merge<br>Merge with parent company dim_customers table |

**Key Transformations:**
- **City Mapping:** Bengaluruu â†’ Bengaluru, Hyderabadd â†’ Hyderabad, etc.
- **Composite Key:** Creates "CustomerName-City" field
- **Static Attributes:** Market (India), Platform (Sports Bar), Channel (Acquisition)

**Output Tables:**
- `fmcg.bronze.customers`
- `fmcg.silver.customers`
- `fmcg.gold.sb_dim_customers` (SportsBar customer dimension)
- Merges into `fmcg.gold.dim_customers` (parent company table)

---

#### `2_products_data_processing.py`
**Purpose:** Process and standardize product dimension data  
**Data Source:** Azure ADLS Gen2 - `products/*.csv`

**Processing:** Similar structure to customer processing
- Bronze â†’ Raw data ingestion
- Silver â†’ Cleaning, standardization, deduplication
- Gold â†’ Business-ready product dimension

---

#### `3_pricing_data_processing.py`
**Purpose:** Process pricing dimension data  
**Data Source:** Azure ADLS Gen2 - `pricing/*.csv`

**Processing:** Standard medallion architecture pipeline for pricing information

---

### 3ï¸âƒ£ Fact Data Processing (3_fact_data_processing/)

#### `1_full_load_fact.py`
**Purpose:** Initial full load of order/transaction facts

**Data Source:** 
- Landing path: `orders/landing/*.csv`
- Processed path: `orders/processed/` (post-ingestion)

**Bronze Layer:**
- Read CSV files from landing directory
- Add metadata columns (file_name, file_size, read_timestamp)
- Append to `fmcg.bronze.orders` with Change Data Feed enabled

**Silver Layer:**
- **Filter:** Keep only rows with non-null `order_qty`
- **Customer ID Cleanup:** Keep numeric IDs, set invalid to "999999"
- **Date Parsing:** Remove weekday names, parse dates in multiple formats
  - Supports: yyyy/MM/dd, dd-MM-yyyy, dd/MM/yyyy, MMMM dd, yyyy
- **Deduplication:** Drop duplicates by (order_id, order_placement_date, customer_id, product_id, order_qty)
- **Type Casting:** Convert product_id to string
- **Join with Products:** Inner join with `fmcg.silver.products` table
- **Merge or Insert:** Uses MERGE logic for upserts into `fmcg.silver.orders`

**Gold Layer:**
- **Aggregation:** Transform daily orders to monthly grain
  - Group by: month_start, product_code, customer_code
  - Aggregate: SUM(sold_quantity)
- **Merge with Parent:** Update parent company's `fmcg.gold.fact_orders` table
- **Output:** `fmcg.gold.sb_fact_orders` (SportsBar orders fact table)

**File Movement:**
- After successful processing, moves files from `orders/landing/` to `orders/processed/`

---

#### `2_incremental_load_fact.py`
**Purpose:** Daily incremental load of new/updated order facts

**Processing:**
- Follows same transformation logic as full load
- Uses MERGE operations for efficient incremental updates
- Supports correction of previous months or status changes
- Handles daily data appends

---

## ğŸ”„ Data Pipeline Flow

### End-to-End Order Processing Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA SOURCES                             â”‚
â”‚  (Spreadsheets, Cloud Drives, APIs, CSV Exports)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    AZURE ADLS Gen2       â”‚
        â”‚  Landing Directories    â”‚
        â”‚ (orders, customers, etc) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                             â”‚
        â–¼                             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ORDERS    â”‚            â”‚  DIMENSIONS  â”‚
    â”‚   PIPELINE  â”‚            â”‚   PIPELINE   â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                          â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     BRONZE      â”‚       â”‚     BRONZE      â”‚
    â”‚  fmcg.bronze.   â”‚       â”‚  fmcg.bronze.   â”‚
    â”‚ orders (raw)    â”‚       â”‚ customers/...   â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                          â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     SILVER      â”‚       â”‚     SILVER      â”‚
    â”‚  fmcg.silver.   â”‚       â”‚  fmcg.silver.   â”‚
    â”‚ orders (clean)  â”‚       â”‚ customers/...   â”‚
    â”‚                 â”‚       â”‚  (clean & std)  â”‚
    â”‚  Joins with     â”‚       â”‚                 â”‚
    â”‚  products       â”‚       â”‚ Merges with     â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ parent tables   â”‚
           â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚        GOLD             â”‚
    â”‚  (Analytics Ready)      â”‚
    â”‚                         â”‚
    â”‚ fmcg.gold.sb_fact_      â”‚
    â”‚ orders (monthly grain)  â”‚
    â”‚                         â”‚
    â”‚ Merges with parent      â”‚
    â”‚ fmcg.gold.fact_orders   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  REPORTING & â”‚
    â”‚  ANALYTICS   â”‚
    â”‚  DASHBOARDS  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Quality Steps at Each Layer

**Bronze:**
- Raw data capture with metadata
- File tracking for auditability

**Silver:**
- Deduplication
- Type standardization
- Null value handling
- Format standardization (dates, text)
- Business rule application
- Join with reference tables

**Gold:**
- Business-level aggregation
- Grain transformation (daily â†’ monthly)
- Merge with parent company data
- Analytics-ready structure

---

## ğŸ”‘ Key Concepts

### Medallion Architecture

```
BRONZE (Raw)
â”œâ”€ Purpose: Data ingestion & lineage
â”œâ”€ Data State: Untransformed, original structure
â”œâ”€ Update Pattern: Append daily
â””â”€ User: Data engineers, auditors

    â–¼

SILVER (Cleaned)
â”œâ”€ Purpose: Single source of truth
â”œâ”€ Data State: Deduplicated, standardized
â”œâ”€ Update Pattern: Merge (upsert)
â””â”€ User: Analytics engineers, analysts

    â–¼

GOLD (Business Ready)
â”œâ”€ Purpose: Analytics & reporting
â”œâ”€ Data State: Aggregated, business logic applied
â”œâ”€ Update Pattern: Merge (upsert)
â””â”€ User: Analysts, BI tools, dashboards
```

### Delta Lake & MERGE Operations
- **ACID Compliance:** Ensures data consistency
- **Schema Enforcement:** Prevents schema drift
- **Change Data Feed:** Tracks changes for auditing
- **Merge Strategy:** Efficiently handles upserts
  - `whenMatchedUpdateAll()` - Update existing records
  - `whenNotMatchedInsertAll()` - Insert new records

### Change Data Feed (CDF)
All tables are created with `delta.enableChangeDataFeed = true`:
```python
.option("delta.enableChangeDataFeed", "true")
```
- Enables tracking of row-level changes
- Useful for change capture and audit trails

---

## ğŸ› ï¸ Implementation Details

### Storage Paths

**ADLS Gen2 Structure:**
```
abfss://container-de-practice@adlsgen2narayan.dfs.core.windows.net/
â”œâ”€â”€ orders/
â”‚   â”œâ”€â”€ landing/        (incoming CSV files)
â”‚   â””â”€â”€ processed/      (completed files)
â”œâ”€â”€ customers/
â”‚   â””â”€â”€ *.csv
â”œâ”€â”€ products/
â”‚   â””â”€â”€ *.csv
â””â”€â”€ pricing/
    â””â”€â”€ *.csv
```

### Database Catalog Structure

```
fmcg (Catalog)
â”œâ”€â”€ bronze (Schema)
â”‚   â”œâ”€â”€ customers
â”‚   â”œâ”€â”€ products
â”‚   â”œâ”€â”€ pricing
â”‚   â””â”€â”€ orders
â”œâ”€â”€ silver (Schema)
â”‚   â”œâ”€â”€ customers
â”‚   â”œâ”€â”€ products
â”‚   â”œâ”€â”€ pricing
â”‚   â””â”€â”€ orders
â””â”€â”€ gold (Schema)
    â”œâ”€â”€ dim_date
    â”œâ”€â”€ dim_customers â†’ sb_dim_customers
    â”œâ”€â”€ dim_products â†’ sb_dim_products
    â”œâ”€â”€ dim_pricing â†’ sb_dim_pricing
    â”œâ”€â”€ sb_fact_orders
    â””â”€â”€ fact_orders (merged parent table)
```

### Parameters & Configuration

**Databricks Widgets (UI Parameters):**
```python
catalog = "fmcg"              # Target catalog
data_source = "customers"     # Data source name (customers, products, orders, etc.)
```

**Key Configurations:**
- Date Range: 2024-01-01 to 2025-12-01 (for date dimension)
- Allowed Cities: ['Bengaluru', 'Hyderabad', 'New Delhi']
- Invalid Customer ID Default: 999999
- Aggregation Grain (Orders): Monthly
- File Format: Delta, Delta Merge enabled

### Data Type Conversions

| Field | Bronze Type | Silver Type | Purpose |
|-------|-------------|------------|---------|
| customer_id | Original | String | Standardization |
| product_id | Original | String | Consistency |
| order_qty | Original | Numeric | Aggregation |
| order_placement_date | String (multiple formats) | Date | Reporting |
| city | String | String (standardized) | Dimension quality |

---

## ğŸ“ˆ Known Issues & Fixes

### Documented Fixes

1. **Customer City Mapping:** Hardcoded fixes for specific customers (IDs: 789403, 789420, 789521, 789603, 789421, 789422, 789522)
   - Status: Business team confirmed
   
2. **Date Parsing:** Multiple format support to handle inconsistent date formats
   - Supports: `yyyy/MM/dd`, `dd-MM-yyyy`, `dd/MM/yyyy`, `MMMM dd, yyyy`
   - Also removes weekday prefix if present

3. **Data Quality:** Dimension reduction based on allowed values
   - City validation against allowed list
   - Customer name standardization

---

## ğŸš€ Deployment & Orchestration

### Workflow Execution
- **Platform:** Databricks Workflows
- **Trigger:** Daily schedule (to be configured)
- **Execution Order:** 
  1. Date dimension creation (if needed)
  2. Dimension processing (customers, products, pricing)
  3. Fact processing (orders full/incremental load)

### Integration with Azure Data Factory
- Configuration file: `factory/nsahu-de-adf.json`
- Supports orchestration of Databricks notebook executions
- Integration Runtime: AutoResolveIntegrationRuntime

---

## ğŸ“ File Movement & Audit Trail

### Order Processing Lifecycle
```
Landing Directory (raw CSV)
    â”‚
    â”œâ”€ Read â†’ Bronze â†’ Silver â†’ Gold
    â”‚
Processed Directory (archive after success)
```

**Metadata Tracked:**
- `read_timestamp` - When data was ingested
- `file_name` - Source file name
- `file_size` - File size in bytes
- Change Data Feed - Row-level changes

---

## ğŸ” Data Quality Metrics

### Deduplication
- **Customer Processing:** Remove duplicates by `customer_id`
- **Order Processing:** Remove duplicates by `(order_id, order_placement_date, customer_id, product_id, order_qty)`

### Validation Rules
- Customer ID: Must be numeric (or default to 999999)
- Order Qty: Must be non-null
- City: Must be in allowed list or null
- Order Date: Must be parseable to date format

---

## ğŸ“š Additional Configuration Files

### Azure Data Factory
- **File:** `factory/nsahu-de-adf.json`
- **Purpose:** Orchestration configuration for Databricks pipeline

### Integration Runtime
- **File:** `integrationRuntime/AutoResolveIntegrationRuntime.json`
- **Purpose:** Compute environment for data integration

### Managed Virtual Network
- **File:** `managedVirtualNetwork/default.json`
- **Purpose:** Network isolation for data factory

### Spark Setup
- **Files:** `Spark/PreReqInstallation/`
  - `InstallationSteps.txt` - Installation guide
  - `SparkOperation.txt` - Operational guidelines

---

## ğŸ“ Next Steps & Recommendations

### Phase 1: Validation
- [ ] Validate data quality in Silver layer
- [ ] Confirm dimension counts match source
- [ ] Verify foreign key relationships in Gold

### Phase 2: Production Readiness
- [ ] Set up automated scheduling in Databricks Workflows
- [ ] Configure alerting for pipeline failures
- [ ] Implement monitoring dashboards
- [ ] Document SLAs for data freshness

### Phase 3: Enhancement
- [ ] Add incremental load optimization
- [ ] Implement data lineage tracking
- [ ] Set up cost optimization monitoring
- [ ] Create user training documentation

---

## ğŸ“ References

- **Project Plan:** `Databricks_project_1/README.MD`
- **Root Documentation:** `README.md`
- **Platform:** Databricks Free Edition / Databricks Community
- **Storage:** Azure Data Lake Storage Gen2 (ADLS Gen2)
- **Delta Lake:** [https://delta.io/](https://delta.io/)

---

**Document Version:** 1.0  
**Last Updated:** December 2, 2025  
**Author:** Documentation Auto-generated
