{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f10a210",
   "metadata": {},
   "source": [
    "# Clinical Intelligence System: RAG-based Capstone Project\n",
    "\n",
    "This notebook demonstrates a Retrieval-Augmented Generation (RAG) pipeline for building a Clinical Intelligence System. The system leverages a set of 100 clinical documents as the exclusive source of truth, and uses advanced retrieval and generation techniques to answer clinical questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e085d776",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "This section loads required libraries, sets up authentication, and prepares the environment for data ingestion and processing. It also defines utility functions for loading and preprocessing CSV data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b39ac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./capstone_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries and modules for RAG pipeline\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    AnswerRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    "    HallucinationMetric,\n",
    "    GEval,\n",
    ")\n",
    "import pandas as pd\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ca8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication and environment variable setup for Azure OpenAI and UHG APIs\n",
    "import httpx\n",
    "auth = \"https://api.uhg.com/oauth2/token\"\n",
    "client_id = dbutils.secrets.get(scope = \"AIML_Training\", key = \"client_id\")\n",
    "client_secret = dbutils.secrets.get(scope = \"AIML_Training\", key = \"client_secret\")\n",
    "scope = \"https://api.uhg.com/.default\"\n",
    "grant_type = \"client_credentials\"\n",
    "async with httpx.AsyncClient() as client:\n",
    "    body = {\n",
    "        \"grant_type\": grant_type,\n",
    "        \"scope\": scope,\n",
    "        \"client_id\": client_id,\n",
    "        \"client_secret\": client_secret,\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    resp = await client.post(auth, headers=headers, data=body, timeout=120)\n",
    "    token = resp.json()[\"access_token\"]\n",
    "\n",
    "\n",
    "load_dotenv(\"./Data/UAIS_vars.env\")\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.environ[\"MODEL_ENDPOINT\"]\n",
    "OPENAI_API_VERSION = os.environ[\"API_VERSION\"]\n",
    "CHAT_DEPLOYMENT_NAME = os.environ[\"CHAT_MODEL_NAME\"]\n",
    "PROJECT_ID = os.environ[\"PROJECT_ID\"]\n",
    "EMBEDDINGS_DEPLOYMENT_NAME = os.environ[\"EMBEDDINGS_MODEL_NAME\"]\n",
    "\n",
    "chat_client = openai.AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_version=OPENAI_API_VERSION,\n",
    "        azure_deployment=CHAT_DEPLOYMENT_NAME,\n",
    "        azure_ad_token=token,\n",
    "        default_headers={\n",
    "            \"projectId\": PROJECT_ID\n",
    "        }\n",
    "    )\n",
    "embeddings_client = openai.AzureOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=OPENAI_API_VERSION,\n",
    "    azure_deployment=EMBEDDINGS_DEPLOYMENT_NAME,\n",
    "    azure_ad_token=token,\n",
    "    default_headers={ \n",
    "        \"projectId\": PROJECT_ID\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d1106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the Environment configuration values are set correctly\n",
    "print(\"Embeddings Client Configuration:\")\n",
    "print(f\"Endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "print(f\"API Version: {OPENAI_API_VERSION}\")\n",
    "print(f\"Deployment Name: {EMBEDDINGS_DEPLOYMENT_NAME}\")\n",
    "print(f\"Model Name: {CHAT_DEPLOYMENT_NAME}\")\n",
    "print(f\"Project ID: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a4869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = AzureChatOpenAI(\n",
    "    openai_api_version=OPENAI_API_VERSION,\n",
    "    azure_deployment=CHAT_DEPLOYMENT_NAME,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    azure_ad_token=token,\n",
    "    default_headers={\"projectId\": PROJECT_ID},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb834e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "\n",
    "def load_csv_with_langchain(csv_path, source_column=None):\n",
    "    # Use LangChain's built-in CSVLoader to load CSV data based on provided columns\n",
    "    if source_column:\n",
    "        loader = CSVLoader(csv_path, source_column=source_column)\n",
    "    else:\n",
    "        loader = CSVLoader(csv_path)\n",
    "    \n",
    "    # Load the CSV into LangChain's document format\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"Successfully loaded {len(documents)} document rows from the CSV.\")\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ee331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI Embeddings client for vectorization of documents\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=EMBEDDINGS_DEPLOYMENT_NAME,\n",
    "    api_version=OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    azure_ad_token=token,\n",
    "    default_headers={\n",
    "        \"projectId\": PROJECT_ID\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1aa955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up tiktoken cache and disable ChromaDB telemetry for compliance\n",
    "tiktoken_cache_dir = os.path.abspath(\"./.setup/tiktoken_cache/\")\n",
    "os.path.abspath(\"./.setup/tiktoken_cache/\")\n",
    "os.environ[\"TIKTOKEN_CACHE_DIR\"] = tiktoken_cache_dir\n",
    "\n",
    "# Due to UHG policies, we have to disable telemetry to use ChromaDB\n",
    "# See here for more information: https://docs.trychroma.com/docs/overview/telemetry\n",
    "os.environ[\"ANONYMIZED_TELEMETRY\"]=\"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create and store embeddings in a local ChromaDB vector store.\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=45, max=120), stop=stop_after_attempt(6))\n",
    "# Updated store_embeddings: load entire docs at once (no batching)\n",
    "def store_embeddings(persist_directory, docs=None):\n",
    "    if not docs:\n",
    "        print('No documents provided')\n",
    "        return None\n",
    "    \n",
    "    if os.path.exists(persist_directory) and os.path.isdir(persist_directory) and os.listdir(persist_directory):\n",
    "        print(f\"Loading existing vector store from {persist_directory}\")\n",
    "        vector_store = Chroma(\n",
    "            persist_directory=persist_directory,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Creating new vector store in {persist_directory}\")\n",
    "        vector_store = Chroma.from_texts(\n",
    "            texts=[doc.page_content for doc in docs],\n",
    "            embedding=embeddings,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        vector_store.persist()\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd720b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_document_name(persist_directory):\n",
    "# Load the vector store to retrieve document IDs\n",
    "    vectorstore = Chroma(\n",
    "            persist_directory=persist_directory,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "        \n",
    "# Returns a set of document source names that have already been processed and embedded.\n",
    "# Used to avoid redundant embedding of the same CSV files.\n",
    "    # Extract metadata from all documents in the store\n",
    "    all_metadatas = vectorstore.get()[\"metadatas\"]\n",
    "    \n",
    "    # Create a set of source file paths from metadata\n",
    "    processed_sources = set()\n",
    "    for metadata in all_metadatas:\n",
    "        if metadata and \"source\" in metadata:\n",
    "            processed_sources.add(metadata[\"source\"])\n",
    "    \n",
    "    return processed_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b1360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_new_csv(csv_path, persist_directory):\n",
    "    \"\"\"Filter out CSV files that have already been processed.\"\"\"\n",
    "    processed_sources = get_processed_document_name(persist_directory)\n",
    "\n",
    "    # Ensure csv_path is a list\n",
    "    if isinstance(csv_path, str):\n",
    "        csv_path = [csv_path]\n",
    "    \n",
    "    # Find CSVs that haven't been processed yet\n",
    "    new_csv = [path for path in csv_path if path not in processed_sources]\n",
    "    \n",
    "    if new_csv:\n",
    "        print(f\"Found {len(new_csv)} new CSVs to process: {new_csv}\")\n",
    "    else:\n",
    "        print(\"No new CSVs to process.\")\n",
    "        \n",
    "    return new_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a96931",
   "metadata": {},
   "source": [
    "## Retrieval Strategies (Semantic, Thresholding, Hybrid, Reranking)\n",
    "\n",
    "This section implements various retrieval strategies to fetch relevant document chunks for a given query. The main approach here is semantic retrieval using vector similarity, but the code is structured to allow for hybrid and advanced strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74064d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic retrieval: fetch top-k most similar document chunks using vector similarity\n",
    "# Can be extended to hybrid retrieval by combining with BM25 or other keyword-based methods\n",
    "def semantic_retrieval(query, vectorstore, top_k=3):\n",
    "    results = vectorstore.similarity_search(query, k=top_k*2)  # fetch more to be safe\n",
    "    unique_results = []\n",
    "    seen_contents = set()\n",
    "\n",
    "    for doc in results:\n",
    "        if doc.page_content not in seen_contents:\n",
    "            unique_results.append(doc)\n",
    "            seen_contents.add(doc.page_content)\n",
    "        if len(unique_results) >= top_k:\n",
    "            break\n",
    "\n",
    "    return unique_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83455018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: BM25 retrieval (strategy 2, if desired)\n",
    "\n",
    "def bm25_rag_pipeline(query, vectorstore, top_k=3):\n",
    "    \"\"\"\n",
    "    Combines semantic and keyword search results for diverse retrieval.\n",
    "    \"\"\"\n",
    "    # Get semantic search results\n",
    "    semantic_results = vectorstore.similarity_search(query, k=top_k*2)\n",
    "    semantic_contents = [doc.page_content for doc in semantic_results]\n",
    "    \n",
    "    # Get keyword search results\n",
    "    bm25_retriever = BM25Retriever.from_texts(\n",
    "        texts=semantic_contents,\n",
    "        embedding=embeddings_client,\n",
    "        k=top_k*2\n",
    "    )\n",
    "    keyword_results = bm25_retriever.get_relevant_documents(query, k=top_k*2)\n",
    "    \n",
    "    # Take half from semantic results\n",
    "    final_results = semantic_results[:top_k//2]\n",
    "    \n",
    "    # Add unique keyword results\n",
    "    for doc in keyword_results:\n",
    "        if len(final_results) >= top_k:\n",
    "            break\n",
    "        if doc.page_content not in semantic_contents:\n",
    "            final_results.append(doc)\n",
    "    \n",
    "    # Fill remaining spots with semantic results\n",
    "    remaining_spots = top_k - len(final_results)\n",
    "    if remaining_spots > 0:\n",
    "        start_idx = len(final_results) - remaining_spots\n",
    "        final_results.extend(semantic_results[start_idx:start_idx+remaining_spots])\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5b1e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an answer using the chat model, strictly based on retrieved context.\n",
    "# The prompt instructs the model to only use the provided context.\n",
    "# If the answer is not found in the context, a fallback message is returned.\n",
    "def generate_answer(query, top_chunks, model_name=CHAT_DEPLOYMENT_NAME):\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in top_chunks])\n",
    "    prompt = (\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {query}\\n\"\n",
    "        f\"Answer (generate an answer strictly based on the above context; do not use your own knowledge. \"\n",
    "        f\"If the query is not covered in the context, respond with: 'No relevant content found in the provided csv data.'):\"\n",
    "    )\n",
    "    \n",
    "    response = chat_client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        model=model_name\n",
    "    )\n",
    "    \n",
    "    gpt_output = response.choices[0].message.content\n",
    "    return gpt_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76a4054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "def get_file_hash(file_path):\n",
    "    \"\"\"Generate a hash for the given file.\"\"\"\n",
    "    hasher = hashlib.md5()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        buf = f.read()\n",
    "        hasher.update(buf)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def csv_chatbot_pipeline(csv_path, user_query, persist_directory):\n",
    "    \"\"\"\n",
    "    Full pipeline: Load CSV → Embed → Retrieve → Generate Answer.\n",
    "    Returns a dictionary with context, question, and AI-generated response.\n",
    "    \"\"\"\n",
    "    csv_hash_path = os.path.join(persist_directory, 'csv_hash.txt')\n",
    "    current_csv_hash = get_file_hash(csv_path)\n",
    "    \n",
    "    if os.path.exists(persist_directory) and os.path.isdir(persist_directory) and os.listdir(persist_directory):\n",
    "        if os.path.exists(csv_hash_path):\n",
    "            with open(csv_hash_path, 'r') as f:\n",
    "                stored_csv_hash = f.read().strip()\n",
    "        else:\n",
    "            stored_csv_hash = None\n",
    "        \n",
    "        if stored_csv_hash == current_csv_hash:\n",
    "            print(f\"Using existing embeddings from {persist_directory}\")\n",
    "            vectorstore = Chroma(\n",
    "                persist_directory=persist_directory,\n",
    "                embedding_function=embeddings\n",
    "            )\n",
    "        else:\n",
    "            print(f\"CSV file has changed. Updating embeddings in {persist_directory}\")\n",
    "            raw_docs = load_csv_with_langchain(csv_path)\n",
    "            vectorstore = Chroma(\n",
    "                persist_directory=persist_directory,\n",
    "                embedding_function=embeddings\n",
    "            )\n",
    "            vectorstore.add_documents(raw_docs)\n",
    "            vectorstore.persist()\n",
    "            with open(csv_hash_path, 'w') as f:\n",
    "                f.write(current_csv_hash)\n",
    "    else:\n",
    "        print(f\"No existing embeddings found. Processing CSV...\")\n",
    "        raw_docs = load_csv_with_langchain(csv_path)\n",
    "        vectorstore = store_embeddings(persist_directory, docs=raw_docs)\n",
    "        os.makedirs(persist_directory, exist_ok=True)\n",
    "        with open(csv_hash_path, 'w') as f:\n",
    "            f.write(current_csv_hash)\n",
    "    \n",
    "    # # Retrieve relevant chunks based on the user query\n",
    "    # retrieved = semantic_retrieval(user_query, vectorstore)\n",
    "    \n",
    "    # Retrieve relevant docs based on the user query using B25\n",
    "    retrieved =bm25_rag_pipeline(user_query, vectorstore)\n",
    "\n",
    "    # Generate the answer using retrieved chunks\n",
    "    answer = generate_answer(user_query, retrieved)\n",
    "    \n",
    "    # Format and return the response\n",
    "    return {\n",
    "        'context': [doc.page_content for doc in retrieved],\n",
    "        'question': user_query,\n",
    "        'AI_generated_response': answer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5779d2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_questions_from_csv(question_csv_path):  \n",
    "    \"\"\"\n",
    "    Load questions from a CSV file. Assumes a column named 'question'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(question_csv_path)\n",
    "    return df['question'].tolist()\n",
    " \n",
    "def batch_generate_answers_from_csv(csv_path, question_csv_path, persist_directory):\n",
    "    \"\"\"\n",
    "    Loads questions from a CSV and generates answers for each.\n",
    "    Returns a list of dicts with question and answer.\n",
    "    \"\"\"\n",
    "    questions = load_questions_from_csv(question_csv_path)\n",
    "    results = []\n",
    "    for q in questions:\n",
    "        pipeline_result = csv_chatbot_pipeline(csv_path, q, persist_directory)\n",
    "        results.append({\n",
    "            'question': q,\n",
    "            'AI_generated_response': pipeline_result['AI_generated_response']\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb1e493",
   "metadata": {},
   "source": [
    "## Validation using capstone1_rag_validation.csv\n",
    "\n",
    "This section validates the RAG pipeline by comparing model-generated answers to human-annotated reference answers from the validation CSV. It uses DeepEval metrics for quantitative evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c31c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for batch mode with questions and existing answers from CSV\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "csv_path = \"./Data/capstone1_rag_dataset.csv\"\n",
    "persist_directory = \"./capstone_croma.db/\"\n",
    "# question_csv_path = \"./Data/capstone1_rag_test_questions.csv\"\n",
    "question_csv_path = \"./Data/capstone1_rag_validation.csv\"\n",
    "# Load questions from CSV\n",
    "questions = load_questions_from_csv(question_csv_path)\n",
    "results = []\n",
    "for q in questions:\n",
    "    pipeline_result = csv_chatbot_pipeline(csv_path, q, persist_directory)\n",
    "    retrieved_docs = pipeline_result['context']  # Assuming context is a list of strings\n",
    "    results.append({\n",
    "        'question': q,\n",
    "        'retrieved_documents': retrieved_docs,\n",
    "        'generated_answer': pipeline_result['AI_generated_response']\n",
    "    })\n",
    "result_df = pd.DataFrame(results)\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860fc643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the required columns for submission\n",
    "submission_df = result_df[['question', 'retrieved_documents', 'generated_answer']]\n",
    "submission_df.to_csv(\"./Data/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3356006",
   "metadata": {},
   "source": [
    "## Test Evaluation using capstone1_rag_test_questions.csv\n",
    "\n",
    "This section demonstrates how to use DeepEval to test model answers against human (manual) answers from the validation CSV. The same approach can be used for the test set by switching the input CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9964d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"DEEPEVAL_TELEMETRY_OPT_OUT\"] = \"YES\"\n",
    "\n",
    "# Wrap AzureChatOpenAI in a compatible wrapper\n",
    "class AzureChatModelWrapper(DeepEvalBaseLLM):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        return self.model.invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return (await self.model.ainvoke(prompt)).content\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"azure-gpt4o-mini\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ddc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap it for DeepEval\n",
    "wrapped_model = AzureChatModelWrapper(chat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676ba2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "from deepeval import evaluate\n",
    "\n",
    "# Load validation data with manual answers\n",
    "validation_df = pd.read_csv('./Data/capstone1_rag_validation.csv')\n",
    "\n",
    "# Align result_df (model output) with validation_df (manual answers) by question\n",
    "test_cases = []\n",
    "for idx, row in validation_df.iterrows():\n",
    "    model_row = result_df[result_df['question'] == row['question']]\n",
    "    if model_row.empty:\n",
    "        continue\n",
    "    model_row = model_row.iloc[0]\n",
    "    retrieved_documents = model_row['retrieved_documents']\n",
    "    if isinstance(retrieved_documents, str):\n",
    "        retrieved_documents = [retrieved_documents]\n",
    "    test_cases.append(\n",
    "        LLMTestCase(\n",
    "            input=row['question'],\n",
    "            actual_output=model_row['generated_answer'],\n",
    "            expected_output=row['reference_answer'],\n",
    "            retrieval_context=retrieved_documents\n",
    "        )\n",
    "    )\n",
    "\n",
    "metric = ContextualPrecisionMetric(\n",
    "    threshold=0.6,\n",
    "    model=wrapped_model,\n",
    "    include_reason=True,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "results = evaluate(test_cases, [metric])\n",
    "\n",
    "# Print summary\n",
    "for i, res in enumerate(results.test_results):\n",
    "    print(f'Q{i+1}:', res.metrics_data[0].success, '| Score:', res.metrics_data[0].score)\n",
    "    print('Reason:', res.metrics_data[0].reason)\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48371ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input='What are the main eye-related symptoms and genetic cause of Wagner syndrome?',\n",
    "    actual_output=result_df.iloc[0]['generated_answer'],\n",
    "    context=[\"described worldwide; about half of these individuals are from the syndrome areas of the eye\"]\n",
    ")\n",
    "\n",
    "metric = HallucinationMetric(\n",
    "    threshold=0.6,\n",
    "    model=wrapped_model,\n",
    "    include_reason=True,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "result = evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c883dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sucess:', result.test_results[0].metrics_data[0].success)\n",
    "print('Score:', result.test_results[0].metrics_data[0].score)\n",
    "print('Reason:', result.test_results[0].metrics_data[0].reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549155b4",
   "metadata": {},
   "source": [
    "## Summary and Best Strategy\n",
    "\n",
    "This notebook implemented a RAG-based Clinical Intelligence System using a set of 100 clinical documents as the exclusive knowledge base. The pipeline includes data loading, embedding, semantic retrieval, response generation, and evaluation using DeepEval metrics.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Semantic retrieval with vector similarity provides strong baseline performance.\n",
    "- The system is easily extensible to hybrid or reranking strategies for improved accuracy.\n",
    "- Evaluation against human-annotated answers enables robust validation of the approach.\n",
    "\n",
    "For best results, consider experimenting with hybrid retrieval and advanced reranking strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
