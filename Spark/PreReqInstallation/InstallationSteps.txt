Spark Installation

Install JDK and do path setup check 
	Using Download JDK
check java version in cmd : javac -version

downlaod Spark witgh hadoop 3
	https://www.apache.org/dyn/closer.lua/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz
Installation : extract the file in specific directory of Spark

Downlaod Winutils:
https://kontext.tech/article/825/hadoop-331-winutils
-> https://github.com/kontext-tech/winutils
download zip file and take which version you want and place in same directory
and place in smae folder the three istalled java, spark and winutils also



add three user paths in environment variable
Like Java-> JavaHome and java path till java folder
same SparkHome and Hadoop home

then edit environment variabvle you can add three variabkles
add in Path variable
Like %JavaHome%\bin
%SparkHome%\bin
%HadoopHome%\bin
 save it

Version Chek:
Java --Version

go to Spark placed directory using command line
then use this command  for check spark: C:\UserInstallation\Spark\bin>spark-shell --master local[3]

 Web link for Local host:  http://BOOK-A85R0IH357:4040


============================ PYTHON Installation ==========================
1. Download Python and instal in local machine
2. Set Environment variables
	a. Path, Pyspark_Python, Pyspark_Driver_Python, PythonPath
3. Run PySpark Shell to verify

PYSARK_PYTHON : Python path \ Python.exe
PYSPARK_DRIVER_PYTHON : python path \ Python.exe
PYTHONPATH : %SPARK_HOME%\python;%SPARK_HOME%\python\lib\*.zip

then for validation: 
in command line : Pyspark <-
 Web link for Local host: http://BOOK-A85R0IH357:4041


First AID program Create Data frame : 
		n = [[1], [2], [3], [4], [5]]
		nDF = spark.createDataFrame(n, "SLNO: int")


