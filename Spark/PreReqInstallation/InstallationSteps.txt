Spark Installation

Install JDK and do path setup check 
	Using Download JDK
check java version in cmd : javac -version

downlaod Spark witgh hadoop 3
	https://www.apache.org/dyn/closer.lua/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz
Installation : extract the file in specific directory of Spark

Downlaod Winutils:
https://kontext.tech/article/825/hadoop-331-winutils
-> https://github.com/kontext-tech/winutils
download zip file and take which version you want and place in same directory
and place in smae folder the three istalled java, spark and winutils also



add three user paths in environment variable
Like Java-> JavaHome and java path till java folder
same SparkHome and Hadoop home

then edit environment variabvle you can add three variabkles
add in Path variable
Like %JavaHome%\bin
%SparkHome%\bin
%HadoopHome%\bin
 save it

Version Chek:
Java --Version

go to Spark placed directory using command line
then use this command  for check spark: C:\UserInstallation\Spark\bin>spark-shell --master local[3]

 Web link for Local host:  http://BOOK-A85R0IH357:4040


============================ PYTHON Installation ==========================
1. Download Python and instal in local machine
2. Set Environment variables
	a. Path, Pyspark_Python, Pyspark_Driver_Python, PythonPath
3. Run PySpark Shell to verify

PYSPARK_PYTHON : Python path \ Python.exe
PYSPARK_DRIVER_PYTHON : python path \ Python.exe
PYTHONPATH : %SPARK_HOME%\python;%SPARK_HOME%\python\lib\*.zip

then for validation: 
in command line : 
python --version
echo %PYSPARK_PYTHON%
echo %PYSPARK_DRIVER_PYTHON%
Pyspark <-


 Web link for Local host: http://BOOK-A85R0IH357:4041


First AID program Create Data frame : 
		n = [[1], [2], [3], [4], [5]]
		nDF = spark.createDataFrame(n, "SLNO: int")

for Result : nDF.show()


============== Using Jupytor notebook =======================
Steps: 
	1. Download and install Anaconda distribution
		link: https://www.anaconda.com/download/success
	2. Setup conda environment
		a. Add Python, FindSpark, Hive, Delta Lake
		b. Add Jupyter notebooks
		Add python version with creating new environment
		click play button -> Open terminal
			=> In temina use below commands
				conda install -c conda-forge findspark
				
				conda install -c conda-forge pyhive delta-spark jupyter
				
	3. Lunch a Jupyter notebook
		in notebook : 
====> Basic steps in jupyter
import findspark #for connect local spark installation

findspark.init() #to initialize spark
findspark.find()

from pyspark.sql import SparkSession
from pyspark.sql.types import *

##create spark application 
spark = (
    SparkSession
    .builder
    .appName("MyFirstDEApp")
    .master("local[4]")
    .getOrCreate()    
)
lunch Application : spark


n = [[1], [2], [3], [4], [5]]
nDF = spark.createDataFrame(n, "SLNO: int")

nDF.show()




